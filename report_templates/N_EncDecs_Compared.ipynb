{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Encoder-Decoders Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report_files = ['/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing6_200_512_04drb/encdec_noing6_200_512_04drb.json','/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing10_200_512_04drb/encdec_noing10_200_512_04drb.json','/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing15_200_512_04drb/encdec_noing15_200_512_04drb.json', '/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing23_200_512_04drb/encdec_noing23_200_512_04drb.json']\n",
    "log_files = ['/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing6_200_512_04drb/encdec_noing6_200_512_04drb_logs.json','/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing10_200_512_04drb/encdec_noing10_200_512_04drb_logs.json','/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing15_200_512_04drb/encdec_noing15_200_512_04drb_logs.json', '/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing23_200_512_04drb/encdec_noing23_200_512_04drb_logs.json']\n",
    "reports = []\n",
    "logs = []\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for report_file in report_files:\n",
    "    with open(report_file) as f:\n",
    "        reports.append((report_file.split('/')[-1].split('.json')[0], json.loads(f.read())))\n",
    "for log_file in log_files:\n",
    "    with open(log_file) as f:\n",
    "        logs.append((log_file.split('/')[-1].split('.json')[0], json.loads(f.read())))\n",
    "        \n",
    "for report_name, report in reports:\n",
    "    print '\\n', report_name, '\\n'\n",
    "    print 'Encoder: \\n', report['architecture']['encoder']\n",
    "    print 'Decoder: \\n', report['architecture']['decoder']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity on Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_table(data):\n",
    "    display(HTML(\n",
    "        u'<table><tr>{}</tr></table>'.format(\n",
    "            u'</tr><tr>'.join(\n",
    "                u'<td>{}</td>'.format('</td><td>'.join(unicode(_) for _ in row)) for row in data)\n",
    "            )\n",
    "    ))\n",
    "\n",
    "def bar_chart(data):\n",
    "    n_groups = len(data)\n",
    "    \n",
    "    train_perps = [d[1] for d in data]\n",
    "    valid_perps = [d[2] for d in data]\n",
    "    test_perps = [d[3] for d in data]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    \n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.3\n",
    "\n",
    "    opacity = 0.4\n",
    "    error_config = {'ecolor': '0.3'}\n",
    "\n",
    "    train_bars = plt.bar(index, train_perps, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='b',\n",
    "                     error_kw=error_config,\n",
    "                     label='Training Perplexity')\n",
    "\n",
    "    valid_bars = plt.bar(index + bar_width, valid_perps, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='r',\n",
    "                     error_kw=error_config,\n",
    "                     label='Valid Perplexity')\n",
    "    test_bars = plt.bar(index + 2*bar_width, test_perps, bar_width,\n",
    "                     alpha=opacity,\n",
    "                     color='g',\n",
    "                     error_kw=error_config,\n",
    "                     label='Test Perplexity')\n",
    "\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Perplexity by Model and Dataset')\n",
    "    plt.xticks(index + bar_width / 3, [d[0] for d in data])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "data = [['<b>Model</b>', '<b>Train Perplexity</b>', '<b>Valid Perplexity</b>', '<b>Test Perplexity</b>']]\n",
    "\n",
    "for rname, report in reports:\n",
    "    data.append([rname, report['train_perplexity'], report['valid_perplexity'], report['test_perplexity']])\n",
    "\n",
    "display_table(data)\n",
    "bar_chart(data[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs. Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 8))\n",
    "for rname, l in logs:\n",
    "    for k in l.keys():\n",
    "        plt.plot(l[k][0], l[k][1], label=str(k) + ' ' + rname + ' (train)')\n",
    "        plt.plot(l[k][0], l[k][2], label=str(k) + ' ' + rname + ' (valid)')\n",
    "plt.title('Loss v. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity vs. Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(10, 8))\n",
    "for rname, l in logs:\n",
    "    for k in l.keys():\n",
    "        plt.plot(l[k][0], l[k][3], label=str(k) + ' ' + rname + ' (train)')\n",
    "        plt.plot(l[k][0], l[k][4], label=str(k) + ' ' + rname + ' (valid)')\n",
    "plt.title('Perplexity v. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_sample(sample, best_bleu=None):\n",
    "    enc_input = ' '.join([w for w in sample['encoder_input'].split(' ') if w != '<pad>'])\n",
    "    gold = ' '.join([w for w in sample['gold'].split(' ') if w != '<mask>'])\n",
    "    print('Input: '+ enc_input + '\\n')\n",
    "    print('Gend: ' + sample['generated'] + '\\n')\n",
    "    print('True: ' + gold + '\\n')\n",
    "    if best_bleu is not None:\n",
    "        cbm = ' '.join([w for w in best_bleu['best_match'].split(' ') if w != '<mask>'])\n",
    "        print('Closest BLEU Match: ' + cbm + '\\n')\n",
    "        print('Closest BLEU Score: ' + str(best_bleu['best_score']) + '\\n')\n",
    "    print('\\n')\n",
    "    \n",
    "def display_sample(samples, best_bleu=False):\n",
    "    for enc_input in samples:\n",
    "        data = []\n",
    "        for rname, sample in samples[enc_input]:\n",
    "            gold = ' '.join([w for w in sample['gold'].split(' ') if w != '<mask>'])\n",
    "            data.append([rname, '<b>Generated: </b>' + sample['generated']])\n",
    "            if best_bleu:\n",
    "                cbm = ' '.join([w for w in sample['best_match'].split(' ') if w != '<mask>'])\n",
    "                data.append([rname, '<b>Closest BLEU Match: </b>' + cbm + ' (Score: ' + str(sample['best_score']) + ')'])\n",
    "        data.insert(0, ['<u><b>' + enc_input + '</b></u>', '<b>True: ' + gold+ '</b>'])\n",
    "        display_table(data)\n",
    "\n",
    "def process_samples(samples):\n",
    "    # consolidate samples with identical inputs\n",
    "    result = {}\n",
    "    for rname, t_samples, t_cbms in samples:\n",
    "        for i, sample in enumerate(t_samples):\n",
    "            enc_input = ' '.join([w for w in sample['encoder_input'].split(' ') if w != '<pad>'])\n",
    "            if t_cbms is not None:\n",
    "                sample.update(t_cbms[i])\n",
    "            if enc_input in result:\n",
    "                result[enc_input].append((rname, sample))\n",
    "            else:\n",
    "                result[enc_input] = [(rname, sample)]\n",
    "    return result\n",
    "\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = process_samples([(rname, r['train_samples'], r['best_bleu_matches_train'] if 'best_bleu_matches_train' in r else None) for (rname, r) in reports])\n",
    "display_sample(samples, best_bleu='best_bleu_matches_train' in reports[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = process_samples([(rname, r['valid_samples'], r['best_bleu_matches_valid'] if 'best_bleu_matches_valid' in r else None) for (rname, r) in reports])\n",
    "display_sample(samples, best_bleu='best_bleu_matches_valid' in reports[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = process_samples([(rname, r['test_samples'], r['best_bleu_matches_test'] if 'best_bleu_matches_test' in r else None) for (rname, r) in reports])\n",
    "display_sample(samples, best_bleu='best_bleu_matches_test' in reports[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_bleu(blue_structs):\n",
    "    data= [['<b>Model</b>', '<b>Overall Score</b>','<b>1-gram Score</b>','<b>2-gram Score</b>','<b>3-gram Score</b>','<b>4-gram Score</b>']]\n",
    "    for rname, blue_struct in blue_structs:\n",
    "        data.append([rname, blue_struct['score'], blue_struct['components']['1'], blue_struct['components']['2'], blue_struct['components']['3'], blue_struct['components']['4']])\n",
    "    display_table(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Set BLEU Scores\n",
    "print_bleu([(rname, report['train_bleu']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validation Set BLEU Scores\n",
    "print_bleu([(rname, report['valid_bleu']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Set BLEU Scores\n",
    "print_bleu([(rname, report['test_bleu']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All Data BLEU Scores\n",
    "print_bleu([(rname, report['combined_bleu']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-pairs BLEU Analysis\n",
    "\n",
    "This analysis randomly samples 1000 pairs of generations/ground truths and treats them as translations, giving their BLEU score. We can expect very low scores in the ground truth and high scores can expose hyper-common generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Set BLEU n-pairs Scores\n",
    "print_bleu([(rname, report['n_pairs_bleu_train']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validation Set n-pairs BLEU Scores\n",
    "print_bleu([(rname, report['n_pairs_bleu_valid']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Set n-pairs BLEU Scores\n",
    "print_bleu([(rname, report['n_pairs_bleu_test']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combined n-pairs BLEU Scores\n",
    "print_bleu([(rname, report['n_pairs_bleu_all']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ground Truth n-pairs BLEU Scores\n",
    "print_bleu([(rname, report['n_pairs_bleu_gold']) for (rname, report) in reports])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Analysis\n",
    "\n",
    "This analysis computs the average Smith-Waterman alignment score for generations, with the same intuition as N-pairs BLEU, in that we expect low scores in the ground truth and hyper-common generations to raise the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_align(reports):\n",
    "    data= [['<b>Model</b>', '<b>Average (Train) Generated Score</b>','<b>Average (Valid) Generated Score</b>','<b>Average (Test) Generated Score</b>','<b>Average (All) Generated Score</b>', '<b>Average (Gold) Score</b>']]\n",
    "    for rname, report in reports:\n",
    "        data.append([rname, report['average_alignment_train'], report['average_alignment_valid'], report['average_alignment_test'], report['average_alignment_all'], report['average_alignment_gold']])\n",
    "    display_table(data)\n",
    "\n",
    "print_align(reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
