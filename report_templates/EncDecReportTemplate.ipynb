{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report_file = 'reports/encdec_200_512_2.json'\n",
    "log_file = 'logs/encdec_200_512_logs.json'\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "with open(report_file) as f:\n",
    "    report = json.loads(f.read())\n",
    "with open(log_file) as f:\n",
    "    logs = json.loads(f.read())\n",
    "print'Encoder: \\n\\n', report['architecture']['encoder']\n",
    "print'Decoder: \\n\\n', report['architecture']['decoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity on Each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Train Perplexity: ', report['train_perplexity'])\n",
    "print('Valid Perplexity: ', report['valid_perplexity'])\n",
    "print('Test Perplexity: ', report['test_perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss vs. Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for k in logs.keys():\n",
    "    plt.plot(logs[k][0], logs[k][1], label=str(k) + ' (train)')\n",
    "    plt.plot(logs[k][0], logs[k][2], label=str(k) + ' (valid)')\n",
    "plt.title('Loss v. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity vs. Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for k in logs.keys():\n",
    "    plt.plot(logs[k][0], logs[k][3], label=str(k) + ' (train)')\n",
    "    plt.plot(logs[k][0], logs[k][4], label=str(k) + ' (valid)')\n",
    "plt.title('Perplexity v. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_sample(sample, best_bleu=None):\n",
    "    enc_input = ' '.join([w for w in sample['encoder_input'].split(' ') if w != '<pad>'])\n",
    "    gold = ' '.join([w for w in sample['gold'].split(' ') if w != '<mask>'])\n",
    "    print('Input: '+ enc_input + '\\n')\n",
    "    print('Gend: ' + sample['generated'] + '\\n')\n",
    "    print('True: ' + gold + '\\n')\n",
    "    if best_bleu is not None:\n",
    "        cbm = ' '.join([w for w in best_bleu['best_match'].split(' ') if w != '<mask>'])\n",
    "        print('Closest BLEU Match: ' + cbm + '\\n')\n",
    "        print('Closest BLEU Score: ' + str(best_bleu['best_score']) + '\\n')\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sample in enumerate(report['train_samples']):\n",
    "    print_sample(sample, report['best_bleu_matches_train'][i] if 'best_bleu_matches_train' in report else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sample in enumerate(report['valid_samples']):\n",
    "    print_sample(sample, report['best_bleu_matches_valid'][i] if 'best_bleu_matches_valid' in report else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, sample in enumerate(report['test_samples']):\n",
    "    print_sample(sample, report['best_bleu_matches_test'][i] if 'best_bleu_matches_test' in report else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_bleu(blue_struct):\n",
    "    print 'Overall Score: ', blue_struct['score'], '\\n'\n",
    "    print '1-gram Score: ', blue_struct['components']['1']\n",
    "    print '2-gram Score: ', blue_struct['components']['2']\n",
    "    print '3-gram Score: ', blue_struct['components']['3']\n",
    "    print '4-gram Score: ', blue_struct['components']['4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Set BLEU Scores\n",
    "print_bleu(report['train_bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation Set BLEU Scores\n",
    "print_bleu(report['valid_bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Set BLEU Scores\n",
    "print_bleu(report['test_bleu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All Data BLEU Scores\n",
    "print_bleu(report['combined_bleu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-pairs BLEU Analysis\n",
    "\n",
    "This analysis randomly samples 1000 pairs of generations/ground truths and treats them as translations, giving their BLEU score. We can expect very low scores in the ground truth and high scores can expose hyper-common generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training Set BLEU n-pairs Scores\n",
    "print_bleu(report['n_pairs_bleu_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validation Set n-pairs BLEU Scores\n",
    "print_bleu(report['n_pairs_bleu_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Set n-pairs BLEU Scores\n",
    "print_bleu(report['n_pairs_bleu_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combined n-pairs BLEU Scores\n",
    "print_bleu(report['n_pairs_bleu_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ground Truth n-pairs BLEU Scores\n",
    "print_bleu(report['n_pairs_bleu_gold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment Analysis\n",
    "\n",
    "This analysis computs the average Smith-Waterman alignment score for generations, with the same intuition as N-pairs BLEU, in that we expect low scores in the ground truth and hyper-common generations to raise the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Average (Train) Generated Score: ', report['average_alignment_train']\n",
    "print 'Average (Valid) Generated Score: ', report['average_alignment_valid']\n",
    "print 'Average (Test) Generated Score: ', report['average_alignment_test']\n",
    "print 'Average (All) Generated Score: ', report['average_alignment_all']\n",
    "print 'Average Gold Score: ', report['average_alignment_gold']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
