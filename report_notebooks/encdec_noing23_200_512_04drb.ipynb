{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": ["# Encoder-Decoder Analysis"], "cell_type": "markdown", "metadata": {}}, {"source": ["### Model Architecture"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["report_file = '/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing23_200_512_04drb/encdec_noing23_200_512_04drb.json'\n", "log_file = '/Users/bking/IdeaProjects/LanguageModelRNN/experiment_results/encdec_noing23_200_512_04drb/encdec_noing23_200_512_04drb_logs.json'\n", "\n", "import json\n", "import matplotlib.pyplot as plt\n", "with open(report_file) as f:\n", "    report = json.loads(f.read())\n", "with open(log_file) as f:\n", "    logs = json.loads(f.read())\n", "print'Encoder: \\n\\n', report['architecture']['encoder']\n", "print'Decoder: \\n\\n', report['architecture']['decoder']"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Perplexity on Each Dataset"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print('Train Perplexity: ', report['train_perplexity'])\n", "print('Valid Perplexity: ', report['valid_perplexity'])\n", "print('Test Perplexity: ', report['test_perplexity'])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Loss vs. Epoch"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%matplotlib inline\n", "for k in logs.keys():\n", "    plt.plot(logs[k][0], logs[k][1], label=str(k) + ' (train)')\n", "    plt.plot(logs[k][0], logs[k][2], label=str(k) + ' (valid)')\n", "plt.title('Loss v. Epoch')\n", "plt.xlabel('Epoch')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Perplexity vs. Epoch"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["%matplotlib inline\n", "for k in logs.keys():\n", "    plt.plot(logs[k][0], logs[k][3], label=str(k) + ' (train)')\n", "    plt.plot(logs[k][0], logs[k][4], label=str(k) + ' (valid)')\n", "plt.title('Perplexity v. Epoch')\n", "plt.xlabel('Epoch')\n", "plt.ylabel('Perplexity')\n", "plt.legend()\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Generations"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["def print_sample(sample, best_bleu=None):\n", "    enc_input = ' '.join([w for w in sample['encoder_input'].split(' ') if w != '<pad>'])\n", "    gold = ' '.join([w for w in sample['gold'].split(' ') if w != '<mask>'])\n", "    print('Input: '+ enc_input + '\\n')\n", "    print('Gend: ' + sample['generated'] + '\\n')\n", "    print('True: ' + gold + '\\n')\n", "    if best_bleu is not None:\n", "        cbm = ' '.join([w for w in best_bleu['best_match'].split(' ') if w != '<mask>'])\n", "        print('Closest BLEU Match: ' + cbm + '\\n')\n", "        print('Closest BLEU Score: ' + str(best_bleu['best_score']) + '\\n')\n", "    print('\\n')\n", "    "], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["for i, sample in enumerate(report['train_samples']):\n", "    print_sample(sample, report['best_bleu_matches_train'][i] if 'best_bleu_matches_train' in report else None)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["for i, sample in enumerate(report['valid_samples']):\n", "    print_sample(sample, report['best_bleu_matches_valid'][i] if 'best_bleu_matches_valid' in report else None)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["for i, sample in enumerate(report['test_samples']):\n", "    print_sample(sample, report['best_bleu_matches_test'][i] if 'best_bleu_matches_test' in report else None)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### BLEU Analysis"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": ["def print_bleu(blue_struct):\n", "    print 'Overall Score: ', blue_struct['score'], '\\n'\n", "    print '1-gram Score: ', blue_struct['components']['1']\n", "    print '2-gram Score: ', blue_struct['components']['2']\n", "    print '3-gram Score: ', blue_struct['components']['3']\n", "    print '4-gram Score: ', blue_struct['components']['4']"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Training Set BLEU Scores\n", "print_bleu(report['train_bleu'])"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Validation Set BLEU Scores\n", "print_bleu(report['valid_bleu'])"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Test Set BLEU Scores\n", "print_bleu(report['test_bleu'])"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# All Data BLEU Scores\n", "print_bleu(report['combined_bleu'])"], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["### N-pairs BLEU Analysis\n", "\n", "This analysis randomly samples 1000 pairs of generations/ground truths and treats them as translations, giving their BLEU score. We can expect very low scores in the ground truth and high scores can expose hyper-common generations"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["# Training Set BLEU n-pairs Scores\n", "print_bleu(report['n_pairs_bleu_train'])"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Validation Set n-pairs BLEU Scores\n", "print_bleu(report['n_pairs_bleu_valid'])"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Test Set n-pairs BLEU Scores\n", "print_bleu(report['n_pairs_bleu_test'])"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Combined n-pairs BLEU Scores\n", "print_bleu(report['n_pairs_bleu_all'])"], "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": ["# Ground Truth n-pairs BLEU Scores\n", "print_bleu(report['n_pairs_bleu_gold'])"], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["### Alignment Analysis\n", "\n", "This analysis computs the average Smith-Waterman alignment score for generations, with the same intuition as N-pairs BLEU, in that we expect low scores in the ground truth and hyper-common generations to raise the scores"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print 'Average (Train) Generated Score: ', report['average_alignment_train']\n", "print 'Average (Valid) Generated Score: ', report['average_alignment_valid']\n", "print 'Average (Test) Generated Score: ', report['average_alignment_test']\n", "print 'Average (All) Generated Score: ', report['average_alignment_all']\n", "print 'Average Gold Score: ', report['average_alignment_gold']"], "outputs": [], "metadata": {"collapsed": true}}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.10", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}